Iteration 1 of 100
Analyze this chess position and suggest the best move.
Current position (FEN): rn2k2r/p1p2p1p/bp2p3/3q2pP/1bP3n1/PP1pPN2/3P1PB1/RNBQK1R1 w Qkq - 2 13
Legal moves: Nxg5, Ne5, Nh4, Nd4, Nh2, Bh3, Bh1, Bf1, Rh1, Rf1, Kf1, Qe2, Qc2, Bb2, Nc3, Ra2, cxd5, axb4, h6, c5, e4, a4
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): r1bqkbr1/p3ppp1/2p1pn1p/8/7P/6PR/PPBn1P2/RNBK2N1 b q - 1 14
Legal moves: Rh8, Kd7, Qd7, Qc7, Qd6, Qb6, Qd5, Qa5, Qd4, Qd3, Bd7, Bb7, Ba6, Rb8, Nh7, Nd7, Nh5, Nd5, Ng4, Nfe4, Nde4+, Nc4+, Nf3+, Nb3+, Nf1+, Nxb1+, g6, a6, h5, e5, c5, g5, a5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): rnbq1b1r/ppppkp2/6pn/4p2p/1PP2P2/3PP1P1/P2K3P/RNBQ1BNR w - - 1 9
Legal moves: Kc3, Ke2, Kc2, Ke1, Nh3, Nf3, Ne2, Bh3, Bg2, Be2, Qxh5, Qg4, Qa4, Qf3, Qb3, Qe2, Qc2, Qe1, Ba3, Bb2, Nc3, Na3, fxe5, f5, c5, b5, g4, e4, d4, h3, a3, h4, a4
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): r1bqkb2/ppppp3/2n2p2/1B4pr/4PP2/2P5/PP1PK1P1/RNB3N1 w q - 0 14
Legal moves: Bxc6, Ba6, Bc4, Ba4, Bd3, Kf3, Ke3, Kd3, Kf2, Kf1, Ke1, Kd1, Nh3, Nf3, Na3, fxg5, f5, e5, c4, g3, d3, b3, a3, g4, d4, b4, a4
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): r1b1k1r1/pp1npp1p/2PpB1p1/8/8/qPb2PPN/PBQP3P/RN3K1R b q - 7 14
Legal moves: Rh8, Rf8, Rg7, Kf8, Kd8, Rb8, Nf8, Nb8, Nf6, Nb6, Ne5, Nc5, Bh8, Bg7, Bf6, Be5, Ba5, Bd4, Bb4, Bxd2, Bxb2, Qa6+, Qc5, Qa5, Qb4, Qa4, Qxb3, Qxb2, Qxa2, fxe6, bxc6, h6, f6, b6, a6, g5, d5, h5, f5, b5, a5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): rnb1k1nr/3pppbp/p5Q1/1Bp3p1/PP6/2q1P3/3P1PPP/RNB1K1NR b KQkq - 0 9
Legal moves: Nh6, Nf6, Kf8, Kd8, Bb7, Nc6, Ra7, Bf8, Bh6, Bf6, Be5, Bd4, Qf6, Qe5, Qd4, Qc4, Qxb4, Qxe3+, Qd3, Qb3, Qa3, Qxd2+, Qc2, Qb2, Qxc1+, Qxa1, hxg6, fxg6, axb5, cxb4, h6, e6, a5, g4, c4, h5, e5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): r4b1r/1pq1kNp1/nBpppn1p/p4Q2/P3P1P1/3P4/1PP2P1P/RN2KB1R b KQ - 2 13
Legal moves: Rg8, Rh7, Re8, Rd8, Rc8, Rb8, Ra7, Ke8, Kxf7, Kd7, Qd8, Qc8, Qb8, Qd7, Qxb6, Ng8, Ne8, Nh7, Nd7, Nh5, Nd5, Nxg4, Nxe4, Nb8, Nc5, Nb4, exf5, g6, h5, e5, d5, c5, g5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): rnbq2nr/2ppppkp/1P1b2p1/R7/1pPP2P1/B3P2N/5PBP/1N1QK1R1 b - - 4 14
Legal moves: Nh6, Nf6, Qf8, Qe8, Bb7, Ba6, Nc6, Na6, Ra7, Ra6, Rxa5, Kf8, Kh6, Kf6, Be5, Bc5, Bf4, Bg3, Bxh2, cxb6, bxa3, h6, f6, e6, c6, g5, b3, h5, f5, e5, c5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): r1b1k2r/pp1pnp1p/q6Q/2p1p1p1/3NP1P1/1P5P/n1PP1PB1/RNB1K2R b KQkq - 0 11
Legal moves: Rg8, Rf8, Kd8, Rb8, Ng8, Ng6, Nc6, Nf5, Nd5, Qxh6, Qg6, Qf6, Qe6, Qd6, Qc6, Qb6, Qb5, Qa5, Qc4, Qa4, Qd3, Qa3, Qe2+, Qf1+, Nb4, Nc3, Nxc1, exd4, cxd4, f6, d6, b6, c4, f5, d5, b5
Please respond with your move in SAN format. Answer:
Analyze this chess position and suggest the best move.
Current position (FEN): rnb2bnr/p2kqppp/3p4/1pp1p3/4P1P1/PP3P2/2PP1KBP/RNBQ2NR b - - 2 7
Legal moves: Nh6, Nf6, Bb7, Ba6, Nc6, Na6, Qe8, Qd8, Qf6, Qe6, Qg5, Qh4+, Ke8, Kd8, Kc7, Ke6, Kc6, h6, g6, f6, a6, d5, c4, b4, h5, g5, f5, a5
Please respond with your move in SAN format. Answer:
Prepared dataset
Prepare Training...
> /Users/chloe/Desktop/deepchess/chess_gym.py(218)<module>()
-> print("Training...")
OptimizedModule(
  (_orig_mod): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): Qwen2ForCausalLM(
        (model): Qwen2Model(
          (embed_tokens): Embedding(151936, 896)
          (layers): ModuleList(
            (0-23): 24 x Qwen2DecoderLayer(
              (self_attn): Qwen2Attention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=896, out_features=896, bias=True)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=896, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=896, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (k_proj): Linear(in_features=896, out_features=128, bias=True)
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=896, out_features=128, bias=True)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=896, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=128, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): Linear(in_features=896, out_features=896, bias=False)
              )
              (mlp): Qwen2MLP(
                (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
                (up_proj): Linear(in_features=896, out_features=4864, bias=False)
                (down_proj): Linear(in_features=4864, out_features=896, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
              (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
            )
          )
          (norm): Qwen2RMSNorm((896,), eps=1e-06)
          (rotary_emb): Qwen2RotaryEmbedding()
        )
        (lm_head): Linear(in_features=896, out_features=151936, bias=False)
      )
    )
  )
)
device(type='mps', index=0)
*** AttributeError: 'Qwen2ForCausalLM' object has no attribute 'precision'
Traceback (most recent call last):
  File "/Users/chloe/Desktop/deepchess/chess_gym.py", line 218, in <module>
    print("Training...")
  File "/Users/chloe/Desktop/deepchess/chess_gym.py", line 218, in <module>
    print("Training...")
  File "/Users/chloe/miniconda3/envs/deepchess/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/Users/chloe/miniconda3/envs/deepchess/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
